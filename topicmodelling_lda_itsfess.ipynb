{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naufalfano/topicmodelling_LDA/blob/main/topicmodelling_lda_itsfess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleansing & Pre-Processing"
      ],
      "metadata": {
        "id": "_5B1VuhxgWQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload file local ke dalam Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "W9ixwYNUgom9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Membaca file yang telah diupload\n",
        "import pandas as pd\n",
        "df = pd.read_excel('twit_fulltext.xlsx')\n",
        "\n",
        "first_10_rows = df.head(10)\n",
        "print(first_10_rows)"
      ],
      "metadata": {
        "id": "I8QbH4KJp-_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert to lowercase**"
      ],
      "metadata": {
        "id": "CtPTHsysrwcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah semua text menjadi lowercase\n",
        "df['full_text'] = df['full_text'].str.lower()\n",
        "\n",
        "first_10_rows = df.head(10)\n",
        "print(first_10_rows)"
      ],
      "metadata": {
        "id": "Ge19vsPprv8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clear URL**"
      ],
      "metadata": {
        "id": "p48gpjCqQmls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Mendefinisikan Regex untuk mengidentifikasi pattern URL\n",
        "pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "# Function untuk menghilangkan URL\n",
        "def remove_urls(text):\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "# Menggunakan function pada kolom 'full_text'\n",
        "column_name = 'full_text'\n",
        "df[column_name] = df[column_name].apply(remove_urls)\n"
      ],
      "metadata": {
        "id": "S7Bo3d3RQlRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clear its/**"
      ],
      "metadata": {
        "id": "uh-SLVvQsA8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Menghilangkan \"its/\" pada awalan text\n",
        "df['full_text'] = df['full_text'].str.replace('its/', '')\n",
        "\n",
        "#Test\n",
        "first_10_rows = df.head(10)\n",
        "print(first_10_rows)"
      ],
      "metadata": {
        "id": "HgvasqlQsAIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make sure all word in alphabet**"
      ],
      "metadata": {
        "id": "GGf5r0nOsPkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Pola regex untuk identifikasi karakter non-alphabet\n",
        "pattern = r'[^a-zA-Z\\s]+'\n",
        "\n",
        "# Function untuk menghapus karakter non-alphabet\n",
        "def remove_non_alphabet(text):\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "# Mengubah value text menjadi string\n",
        "df['full_text'] = df['full_text'].apply(lambda x: str(x) if not isinstance(x, str) else x)\n",
        "\n",
        "# Menggunakan function pada kolom 'full_text'\n",
        "df['full_text'] = df['full_text'].apply(remove_non_alphabet)\n",
        "\n",
        "#Test\n",
        "first_10_rows = df.head(10)\n",
        "print(first_10_rows)"
      ],
      "metadata": {
        "id": "irF1yVUYsRs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Punctuation**"
      ],
      "metadata": {
        "id": "dedkHjINBj4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Regex function untuk menghilangkan tanda baca\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Menggunakan function pada kolom 'full_text'\n",
        "df['full_text'] = df['full_text'].apply(remove_punctuation)\n",
        "\n",
        "#Test\n",
        "first_10_rows = df.head(10)\n",
        "print(first_10_rows)\n"
      ],
      "metadata": {
        "id": "upAuRLDCBluX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Leading Space & Extra Whitespace**"
      ],
      "metadata": {
        "id": "0lHZEuM2A2j4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghilangkan leading space\n",
        "df['full_text'] = df['full_text'].str.lstrip()\n",
        "\n",
        "# Function untuk menghilangkan extra whitespace pada text\n",
        "def remove_extra_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Apply the function to the specified column.\n",
        "df['full_text'] = df['full_text'].apply(remove_extra_spaces)\n",
        "\n",
        "# Test\n",
        "first_10_rows = df.head(10)\n",
        "print(first_10_rows)"
      ],
      "metadata": {
        "id": "E6aZe_XIA2Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove 1 word item**"
      ],
      "metadata": {
        "id": "-6sXRjrmh5Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function untuk menghilangkan kalimat yang hanya terdiri dari 1 kata\n",
        "def remove_single_word(text):\n",
        "    sentences = text.split('.')\n",
        "    filtered_sentences = [sentence.strip() for sentence in sentences if len(sentence.split()) > 1]\n",
        "    return '. '.join(filtered_sentences)\n",
        "\n",
        "# Apply the function to the specified column.\n",
        "df['full_text'] = df['full_text'].apply(remove_single_word)\n",
        "\n",
        "# Test\n",
        "first_10_rows = df.head(10)\n",
        "print(first_10_rows)"
      ],
      "metadata": {
        "id": "Nq0PFeaUh9Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove additional stopword**"
      ],
      "metadata": {
        "id": "6T9H1idCCCai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function untuk menghilangkan additional stopwords\n",
        "def remove_additional_stopwords(text):\n",
        "    # Additional stopwords yang akan dihilangkan\n",
        "    words_to_remove = [\"arek\", \"rek\", \"mas\", \"mba\", \"mbak\", \"rt\", \"ga\", \"g\", \"ada\", \"yg\", \"yang\", \"dong\", \"sby\", \"surabaya\", \"titipan\", \"cowo\", \"cewe\", \"ya\", \"y\", \"km\", \"rekk\", \"apa\", \"gasi\"]\n",
        "\n",
        "    # Regex pattern untuk menyesuaikan kalimat\n",
        "    pattern = r'\\b(?:' + '|'.join(words_to_remove) + r')\\b'\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "# Apply the function to the specified column.\n",
        "df['full_text'] = df['full_text'].apply(remove_additional_stopwords)"
      ],
      "metadata": {
        "id": "ac8FixqGCOcN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Stopword & Tokenizing**"
      ],
      "metadata": {
        "id": "J7sKwnTcBOq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('indonesian'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "tokenized_text = 'full_text'\n",
        "# Apply the function to the 'full_text' column\n",
        "df['tokenized_text'] = df['full_text'].apply(tokenize_and_remove_stopwords)\n",
        "\n",
        "print(df[['full_text', 'tokenized_text']])"
      ],
      "metadata": {
        "id": "cBiEa8xw2j9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Null Value in Column**"
      ],
      "metadata": {
        "id": "xYQBtCY_jghg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable check null value\n",
        "columns_to_check = ['full_text']\n",
        "\n",
        "# Menghapus baris yang memiliki null value\n",
        "df = df.dropna(subset=columns_to_check)"
      ],
      "metadata": {
        "id": "Vah4k6OIjlaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export**"
      ],
      "metadata": {
        "id": "rIwKmQs_s1PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export hasil cleansing (.csv)\n",
        "output_file = 'twit_cleaned.csv'\n",
        "\n",
        "# Menyimpan dataframe menjadi csv\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f'DataFrame has been saved to {output_file}')"
      ],
      "metadata": {
        "id": "381nliWKs4B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Dirichlet Allocation Model"
      ],
      "metadata": {
        "id": "VHzXKwYXBLbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import File**"
      ],
      "metadata": {
        "id": "zn93Ov8EOFhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import file local kedalam Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "-zXXR9ppOJUC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "dfa256c5-ed28-4407-d1bf-26c3041df597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-55601474-648c-4225-9b52-48cbae48bfbb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-55601474-648c-4225-9b52-48cbae48bfbb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving twit_cleaned.csv to twit_cleaned (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Membaca file csv\n",
        "import pandas as pd\n",
        "df = pd.read_csv('twit_cleaned.csv')\n",
        "\n",
        "text = df['full_text']\n",
        "\n",
        "# Memastikan tidak ada value yang hilang\n",
        "df['full_text'] = df['full_text'].fillna('')\n",
        "\n",
        "# Mengubah teks menjadi list\n",
        "text_list = [text.split() for text in df['full_text']]\n",
        "\n",
        "\n",
        "first_10_rows = df.head(10)\n",
        "print(first_10_rows)"
      ],
      "metadata": {
        "id": "KEU3CK-DONLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Bigram & Trigram Model**"
      ],
      "metadata": {
        "id": "L2Y20ryGOSfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# Membuat model bigram & trigram\n",
        "from gensim.models import Phrases\n",
        "\n",
        "# Menambahkan bigram dan trigram yang muncul <=10 ke dalam document\n",
        "bigram = Phrases(text_list, min_count=10)\n",
        "trigram = Phrases(bigram[text_list])\n",
        "for idx in range(len(text_list)):\n",
        "    # Memeriksa bigram dan trigram yang ditandai dengan identifier \"_\"\n",
        "    for token in bigram[text_list[idx]]:\n",
        "        if '_' in token:\n",
        "            text_list[idx].append(token)\n",
        "    for token in trigram[text_list[idx]]:\n",
        "        if '_' in token:\n",
        "            text_list[idx].append(token)"
      ],
      "metadata": {
        "id": "sD8Ibxt_OWT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Dictionary**"
      ],
      "metadata": {
        "id": "Jo3PIAouOdKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "# Membuat\n",
        "dictionary = corpora.Dictionary(text_list)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.2)\n",
        "\n",
        "#no_below (int, optional) – Menyimpan token yang muncul sebanyak <=\"no_below\" dalam document.\n",
        "#no_above (float, optional) – Menyimpan token yang muncul sebanyak >=\"no_above\" dalam document. (nilai pecahan dari ukuran total corpus, bukan angka absolut).\n",
        "print(dictionary)"
      ],
      "metadata": {
        "id": "_9nloAuuOgnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Corpus**"
      ],
      "metadata": {
        "id": "K1fSxlN9Ol_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://radimrehurek.com/gensim/tut1.html\n",
        "# Build corpus\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_list]\n",
        "\n",
        "# The function doc2bow converts document (a list of words) into the bag-of-words format\n",
        "'''The function doc2bow() simply counts the number of occurrences of each distinct word,\n",
        "converts the word to its integer word id and returns the result as a sparse vector.\n",
        "The sparse vector [(0, 1), (1, 1)] therefore reads: in the document “Human computer interaction”,\n",
        "the words computer (id 0) and human (id 1) appear once;\n",
        "the other ten dictionary words appear (implicitly) zero times.'''\n",
        "\n",
        "print(len(doc_term_matrix))\n",
        "print(doc_term_matrix[100])\n",
        "tfidf = models.TfidfModel(doc_term_matrix) #build TF-IDF model\n",
        "corpus_tfidf = tfidf[doc_term_matrix]"
      ],
      "metadata": {
        "id": "WfDwof78OuoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coherence Analysis**"
      ],
      "metadata": {
        "id": "VvZBLxvYO0Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from numpy import array\n",
        "\n",
        "# Function untuk menghitung coherence value\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, iterations=100)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "\n",
        "start=1\n",
        "limit=21\n",
        "step=1\n",
        "model_list, coherence_values = compute_coherence_values(dictionary, corpus=corpus_tfidf,\n",
        "                                                        texts=text_list, start=start, limit=limit, step=step)\n",
        "# Visualisasi data\n",
        "import matplotlib.pyplot as plt\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Print coherence value\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 6))"
      ],
      "metadata": {
        "id": "Hg17r_YFO4nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build Topic Model**"
      ],
      "metadata": {
        "id": "Bnt30jFEPEgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membangun model dengan jumlah topik berdasarkan hasil coherence score\n",
        "model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=20)\n",
        "for idx, topic in model.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))"
      ],
      "metadata": {
        "id": "pXDPo4hXPHkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save top-word to CSV**"
      ],
      "metadata": {
        "id": "yhRq4F2hPLWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import top_word (Kata yang sering muncul dalam setiap topik) dalam bentuk CSV\n",
        "import pandas as pd\n",
        "top_words_per_topic = []\n",
        "for t in range(model.num_topics):\n",
        "    top_words_per_topic.extend([(t, ) + x for x in model.show_topic(t, topn = 10)])\n",
        "df = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word','P']).to_csv(\"top_words.csv\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "h22Hn29gPOdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization**"
      ],
      "metadata": {
        "id": "-DuQtGTzPTQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "# Visualisasi hasil topic modelling\n",
        "data = gensimvis.prepare(model, corpus_tfidf, dictionary)\n",
        "print(data)\n",
        "\n",
        "pyLDAvis.display(data)\n"
      ],
      "metadata": {
        "id": "-4GpmyN3PWsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}